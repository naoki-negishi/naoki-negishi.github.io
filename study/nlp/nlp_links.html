<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device‐width, initial‐scale=1.0">
    <title>Naoki Negishi's homepage</title>
    <style>@import "../../style.css";</style>
    <link rel="icon" href="../gazou/ariete.jpg">
</head>

<body background="../../gazou/haikei.gif" oncontextmenu="alert('右クリックは禁止です！');return false">
    <header id="header">
        <center>
            <h1>NLP 解説サイトリンク集</h1>
            <h3>こちらは個人的にブックマークした NLP の解説サイトを紹介するページです</h3>
        </center>
        <hr>
        <nav id=links>
            <h1>ページ内リンク</h1>
            <ul>
                <li><a href="#category1">概観</a></li>
                <li><a href="#category2">概観2</a></li>
                <li><a href="#category3">アーキテクチャごと</a></li>
                <li><a href="#category4">やや専門的?</a></li>
            </ul>
            <a href="https://naoki-negishi.github.io/top/">トップに戻る Go to top page</a>
        </nav>
    </header>
    <hr>
    <main id="page-body">
        <section id="category1">
            <h3 style="background-color:#d8d088">
                <font color="#330000">◇ 概観</font></h3>
            <ul><dl>
                <li><a href="https://speakerdeck.com/chokkan/20230327_riken_llm">
                        <strong>大規模言語モデルの驚異と脅威 - Speaker Deck</strong></a> (2023/3/28)<br>
                    岡崎直観氏による資料</li>
                <li><a href="https://speakerdeck.com/chokkan/llm">
                        <strong>大規模言語モデル - Speaker Deck</strong></a> (2023/9/4)<br>
                    岡崎直観氏による資料2、上の資料よりも詳しい</li>
                <li><a href="https://speakerdeck.com/kyoun/deim-tutorial-part-1-nlp">
                        <strong>NLPとVision-and-Languageの基礎・最新動向 (1) / DEIM Tutorial Part 1: NLP</strong></a></li>
                <li><a href="https://speakerdeck.com/kyoun/deim-tutorial-part-2-vision-and-language">
                        <strong>NLPとVision-and-Languageの基礎・最新動向 (2) / DEIM Tutorial Part 2 Vision-and-Language</strong></a></li>
                <li><a href="https://note.com/it_navi/n/nd1d5bd16531f">
                        <strong>最近、人工知能による自然言語処理が爆発的に進化しているのでまとめてみた。【前編】｜IT navi｜note</strong></a>
                </li>
            </dl></ul>
        </section>

        <section id="category2">
            <h3 style="background-color:#d8d088">
                <font color="#330000">◇ 概観2</font></h3>
            <p>
                面倒になってきたのでサマリーはChatGPTによるものです。<br>
                あんまり真に受けないでください。<br>
            </p>
            <ul><dl>
                <li><a href="https://speakerdeck.com/11royamasaki/zi-ran-yan-yu-sheng-cheng-xi-ainoxi-pu">
                        <strong>自然言語生成系AIの系譜</strong></a><br>
                    自然言語生成に関するAI技術の発展をまとめた資料</li>
                <li><a href="https://tech-blog.abeja.asia/entry/chat-gpt-first-half-202307">
                        <strong>ChatGPT の仕組みを理解する（前編） - ABEJA Tech Blog</strong></a><br>
                    ChatGPTの基本的な仕組みと技術的背景を解説</li>
                <li><a href="https://speakerdeck.com/verypluming/dong-jing-da-xue-shen-ceng-xue-xi-deep-learningji-chu-jiang-zuo-2022-shen-ceng-xue-xi-tozi-ran-yan-yu-chu-li">
                        <strong>東京大学深層学習（Deep Learning基礎講座2022）深層学習と自然言語処理 - 谷中瞳さん</strong></a><br>
                    深層学習と自然言語処理に関する基礎講座の資料</li>
                <li><a href="https://note.com/it_navi/n/nd1d5bd16531f">
                        <strong>最近、人工知能による自然言語処理が爆発的に進化しているのでまとめてみた。【前編】</strong></a><br>
                    自然言語処理技術の進化についてまとめた記事</li>
                <li><a href="https://deeplearning.hatenablog.com/entry/menhera_chan">
                        <strong>メンヘラちゃんと学ぶディープラーニング最新論文</strong></a><br>
                    メンヘラちゃんと一緒に学べる最新のディープラーニング論文紹介</li>
                <li><a href="https://yagami12.hatenablog.com/entry/2017/12/30/175113">
                        <strong>自然言語処理（NLP）- 星の本棚</strong></a><br>
                    自然言語処理に関する基礎的な解説記事</li>
                <li><a href="https://qiita.com/kouhara/items/e895f6350aa1ebe77133">
                        <strong>自然言語処理（NLP）用語集（論文） - Qiita</strong></a><br>
                    自然言語処理に関する用語をまとめた用語集</li>
                <li><a href="https://radiology-nlp.hatenablog.com/entry/2019/11/16/005550">
                        <strong>自然言語処理タスクを概観する(1) 文書分類とその変形</strong></a></li>
            </dl></ul>
        </section>

        <section id="category3">
            <h3 style="background-color:#d8d088">
                <font color="#330000">◇ アーキテクチャごと</font></h3>
            <ul><dl>
                <li><a href="https://eieito.hatenablog.com/entry/2022/02/15/100000">
                        <strong>ngram言語モデルについてまとめる (add-one) - エイエイレトリック</strong></a><br>
                    n-gram 言語モデルの基本概念とadd-oneスムージングについて解説</li>
                <li><a href="https://blog.octopt.com/rnn/">
                        <strong>再帰型ニューラルネットワーク（RNN) とは - 解説 OctOpt 技術ブログ</strong></a><br>
                    RNNの構造と基本的な動作について説明</li>
                <li><a href="https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca">
                        <strong>LSTMネットワークの概要 - Qiita</strong></a><br>
                    LSTMの仕組みとその長期依存関係の処理能力について解説</li>
                <li><a href="https://blog.octopt.com/sequence-to-sequence/">
                        <strong>Sequence To Sequence(Seq2Seq) - アルゴリズム解説</strong></a><br>
                    Seq2Seqモデルの構造と使い方について詳細に説明</li>
                <li><a href="https://web.archive.org/web/20220327004921/https://www.renom.jp/ja/notebooks/tutorial/time_series/jp-en_nmt_seq2seq/notebook.html">
                        <strong>seq2seqによる日英機械翻訳</strong></a><br>
                    Seq2Seqを用いた日英翻訳モデルの実装例を紹介</li>
                <li><a href="https://qiita.com/ymym3412/items/c84e6254de89c9952c55">
                        <strong>Seq2Seq+Attentionのその先へ - Qiita</strong></a><br>
                    Attention機構を取り入れたSeq2Seqモデルの進化について解説</li>
                <li><a href="https://tips-memo.com/translation-jayalmmar-attention">
                        <strong>【世界一分かりやすい解説】Attentionを用いたseq2seqのメカニズムBeginaid</strong></a><br>
                    Attention機構を用いたSeq2Seqモデルのメカニズムについて詳細に解説</li>
                <li><a href="https://blog.octopt.com/transformer/">
                        <strong>Transformer (トランスフォーマー） - OctOpt 技術ブログ</strong></a><br>
                    Transformerモデルの構造と動作原理について解説</li>
                <li><a href="https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu">
                        <strong>30分で完全理解するTransformerの世界</strong></a><br>
                    Transformerモデルの概念を短時間で理解できる解説</li>
                <li><a href="https://bbycroft.net/llm">
                        <strong>LLM Visualization</strong></a><br>
                    GPTを含む大規模言語モデルの視覚的な理解を助けるツールを紹介</li>
                <li><a href="https://qiita.com/omiita/items/72998858efc19a368e50">
                        <strong>自然言語処理の王様「BERT」の論文を徹底解説 - Qiita</strong></a><br>
                    BERT論文の解説記事</li>
                <li><a href="https://jalammar.github.io/illustrated-transformer/">
                        <strong>The Illustrated Transformer</strong></a><br>
                    視覚的に理解できるかもしれない</li>
                <li><a href="https://deeplearning.hatenablog.com/entry/transformer">
                        <strong>論文解説 Attention Is All You Need (Transformer) - ディープラーニングブログ</strong></a><br>
                    </li>
                <li><a href="https://www.nogawanogawa.com/entry/is_attention_really_needed%3F">
                        <strong>"Attention is All You Need" は本当か調べる - Re:ゼロから始めるML生活</strong></a><br>
                    CV系を中心？にTransformerのキモがAttentionではないかもしれない文献を紹介している</li>
                <li><a href="https://www.docswell.com/s/DeepLearning2023/ZNRE7Q-2024-06-26-095347">
                        <strong>【拡散モデル勉強会】Introduction to Diffusion Models </strong></a><br>
                    </li>
                <!-- <li><a href=""> -->
                <!--         <strong></strong></a><br> -->
                <!--     </li> -->
                <!-- <li><a href=""> -->
                <!--         <strong></strong></a><br> -->
                <!--     </li> -->
                <!-- <li><a href=""> -->
                <!--         <strong></strong></a><br> -->
                <!--     </li> -->
                <!-- <li><a href=""> -->
                <!--         <strong></strong></a><br> -->
                <!--     </li> -->
            </dl></ul>
        </section>

        <section id="category4">
            <h3 style="background-color:#d8d088">
                <font color="#330000">◇ やや専門的?</font></h3>
            <ul>
                <li><a href="https://speakerdeck.com/eumesy/optimal-transport-for-natural-language-processing">
                        <strong>最適輸送と自然言語処理 - 横井祥さん</strong></a><br>
                    最適輸送理論を自然言語処理に応用する方法についての解説</li>
                <li><a href="https://speakerdeck.com/eumesy/chatgpt-and-intro-of-ot-for-nlp">
                        <strong>ChatGPT と自然言語処理 / 言語の意味の計算と最適輸送</strong></a><br>
                    ChatGPTの自然言語処理における最適輸送理論の役割についての紹介</li>
                <li><a href="https://www.slideshare.net/joisino/ss-251328369">
                        <strong>最適輸送入門 - 佐藤竜馬さん</strong></a><br>
                    最適輸送理論の基本概念とその応用についての入門的な説明</li>
                <li><a href="https://speakerdeck.com/kyoun/a-talk-on-machine-reading-comprehension-nlp2019">
                        <strong>機械読解の現状と展望 / A Talk on Machine Reading Comprehension (NLP2019) - 西田京介さん</strong></a><br>
                    機械読解技術の現状と今後の展望についての講演資料</li>
            </ul>
        </section>
    </main>
    <hr>
    <footer id="footer">
        何か問題がございましたらご連絡ください。すぐに対応いたします。<br>
        © 2021 Naoki Negishi<br>
        <a href="https://naoki-negishi.github.io/top/">トップに戻る Go to top page</a>
    </footer>
</body>

</html>
